{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZvq8WctlkyR"
      },
      "source": [
        "# Proximal Policy Optimization (PPO) Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZSAUgK9l4ZJ"
      },
      "source": [
        "## 1. Environment Preparation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D77XeHmdmYOB"
      },
      "source": [
        "### 1.1 Download Packages for BipedalWalker-v3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6Wj5AJRmVoo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ed72376-22b2-42de-a041-043afbccfc38"
      },
      "source": [
        "!apt-get install -y swig\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install colabgymrender==1.0.2\n",
        "!pip install box2d-py\n",
        "!pip install Box2D gym\n",
        "!pip install gym[Box_2D]\n",
        "# !pip install 'gym[Box2D]'\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (1,711 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 126281 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting colabgymrender==1.0.2\n",
            "  Downloading colabgymrender-1.0.2.tar.gz (1.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.11/dist-packages (from colabgymrender==1.0.2) (3.0)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (from colabgymrender==1.0.2) (1.0.3)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (from colabgymrender==1.0.2) (0.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from colabgymrender==1.0.2) (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym->colabgymrender==1.0.2) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym->colabgymrender==1.0.2) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym->colabgymrender==1.0.2) (0.0.8)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy->colabgymrender==1.0.2) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy->colabgymrender==1.0.2) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy->colabgymrender==1.0.2) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy->colabgymrender==1.0.2) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy->colabgymrender==1.0.2) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy->colabgymrender==1.0.2) (0.6.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio<3.0,>=2.5->moviepy->colabgymrender==1.0.2) (11.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy->colabgymrender==1.0.2) (2025.6.15)\n",
            "Building wheels for collected packages: colabgymrender\n",
            "  Building wheel for colabgymrender (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colabgymrender: filename=colabgymrender-1.0.2-py3-none-any.whl size=2427 sha256=b5bb2656b3ed3fa456569878a909879091138cc15e3574bf0fb0c70d97d00b0d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/cc/b3/535ab5f86aef7106a855e71994084f82a435097ef92a3f7e5a\n",
            "Successfully built colabgymrender\n",
            "Installing collected packages: colabgymrender\n",
            "Successfully installed colabgymrender-1.0.2\n",
            "Collecting box2d-py\n",
            "  Downloading box2d-py-2.3.8.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.5/374.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.8-cp311-cp311-linux_x86_64.whl size=2351185 sha256=ba5c57f7ea3f159c140fca5f6c9f411b7860def3803e6025ba658d42557fdd9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/95/02/4cb5adc9f6dcaeb9639c2271f630a66ab4440102414804c45c\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Collecting Box2D\n",
            "  Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Downloading Box2D-2.3.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Box2D\n",
            "Successfully installed Box2D-2.3.10\n",
            "Requirement already satisfied: gym[Box_2D] in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "\u001b[33mWARNING: gym 0.25.2 does not provide the extra 'box-2d'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym[Box_2D]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym[Box_2D]) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym[Box_2D]) (0.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhbbMMfbnkCD"
      },
      "source": [
        "### 1.2 Mount Drive and Set Project Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEbRp03dmKR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf8f449f-0bc8-4ee1-fc51-be4cfff0b3ae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "project_root = '/content/drive/My Drive/ppo_tutorial/'\n",
        "sys.path.append(project_root)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KGagltVqdFw"
      },
      "source": [
        "### 1.3 Test the BipedalWalker-v3 Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSOEbSc7nNaB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd5e7e6e-d9aa-4d5b-deff-6a4b85e03446"
      },
      "source": [
        "import os\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "env = gym.make('BipedalWalker-v3')\n",
        "s_dim = env.observation_space.shape[0]\n",
        "a_dim = env.action_space.shape[0]\n",
        "print(s_dim)\n",
        "print(a_dim)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/moviepy/config_defaults.py:1: DeprecationWarning: invalid escape sequence '\\P'\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: DeprecationWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please import `sobel` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n",
            "WARNING:py.warnings:<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "\n",
            "WARNING:py.warnings:<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "\n",
            "WARNING:py.warnings:<frozen importlib._bootstrap>:241: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsbCyyGdm7tR"
      },
      "source": [
        "## 2. Policy Network & Value Network Construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZrbzM1S0PGg"
      },
      "source": [
        "### 2.1 Diagonal Gaussian Distribution Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7Xy2egcnB9H"
      },
      "source": [
        "#AddBias module\n",
        "class AddBias(nn.Module):\n",
        "    def __init__(self, bias):\n",
        "        super(AddBias, self).__init__()\n",
        "        self._bias = nn.Parameter(bias.unsqueeze(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        bias = self._bias.t().view(1, -1)\n",
        "        return x + bias\n",
        "\n",
        "#Gaussian distribution with given mean & std.\n",
        "class FixedNormal(torch.distributions.Normal):\n",
        "    def log_probs(self, x):\n",
        "        return super().log_prob(x).sum(-1)\n",
        "\n",
        "    def entropy(self):\n",
        "        return super().entropy().sum(-1)\n",
        "\n",
        "    def mode(self):\n",
        "        return self.mean\n",
        "\n",
        "#Diagonal Gaussian module\n",
        "class DiagGaussian(nn.Module):\n",
        "    def __init__(self, inp_dim, out_dim):\n",
        "        super(DiagGaussian, self).__init__()\n",
        "        self.fc_mean = nn.Linear(inp_dim, out_dim)\n",
        "        self.b_logstd = AddBias(torch.zeros(out_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = self.fc_mean(x)\n",
        "        logstd = self.b_logstd(torch.zeros_like(mean))\n",
        "        return FixedNormal(mean, logstd.exp())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDBBOmDe3Unn"
      },
      "source": [
        "### 2.2 Policy Network & Value Network Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPRvKWq7nZup"
      },
      "source": [
        "#Policy Network\n",
        "class PolicyNet(nn.Module):\n",
        "    #Constructor\n",
        "    def __init__(self, s_dim, a_dim):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(s_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.dist = DiagGaussian(128, a_dim)\n",
        "        #TODO(Lab-1): Policy Network Architecture\n",
        "\n",
        "    #Forward pass\n",
        "    def forward(self, state, deterministic=False):\n",
        "        feature = self.main(state)\n",
        "        dist = self.dist(feature)\n",
        "\n",
        "        if deterministic:\n",
        "            action = dist.mode()\n",
        "        else:\n",
        "            action = dist.sample()\n",
        "\n",
        "        return action, dist.log_probs(action)\n",
        "\n",
        "    #Choose an action (stochastically or deterministically)\n",
        "    def choose_action(self, state, deterministic=False):\n",
        "        feature = self.main(state)\n",
        "        dist = self.dist(feature)\n",
        "\n",
        "        if deterministic:\n",
        "            return dist.mode()\n",
        "\n",
        "        return dist.sample()\n",
        "\n",
        "    #Evaluate a state-action pair (output log-prob. & entropy)\n",
        "    def evaluate(self, state, action):\n",
        "        feature = self.main(state)\n",
        "        dist = self.dist(feature)\n",
        "        return dist.log_probs(action), dist.entropy()\n",
        "\n",
        "#Value Network\n",
        "class ValueNet(nn.Module):\n",
        "    #Constructor\n",
        "    def __init__(self, s_dim):\n",
        "        super(ValueNet, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(s_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "            )\n",
        "\n",
        "    #Forward pass\n",
        "    def forward(self, state):\n",
        "        return self.main(state)[:, 0]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPZG8h2u7iEY"
      },
      "source": [
        "### 2.3 Create Policy Network & Value Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bccYpNWnmxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88287b8f-e791-4191-e0e6-9b431030ee45"
      },
      "source": [
        "policy_net = PolicyNet(s_dim, a_dim)\n",
        "value_net = ValueNet(s_dim)\n",
        "print(policy_net)\n",
        "print(value_net)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PolicyNet(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=24, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (dist): DiagGaussian(\n",
            "    (fc_mean): Linear(in_features=128, out_features=4, bias=True)\n",
            "    (b_logstd): AddBias()\n",
            "  )\n",
            ")\n",
            "ValueNet(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=24, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiAHu2dtoANa"
      },
      "source": [
        "## 3. Environment Runner Construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIkfaD4GWtGh"
      },
      "source": [
        "### 3.1 EnvRunner Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Z0lT_8oKfo"
      },
      "source": [
        "class EnvRunner:\n",
        "    #Constructor\n",
        "    def __init__(self, s_dim, a_dim, gamma=0.99, lamb=0.95, max_step=2048, device='cpu'):\n",
        "        self.s_dim = s_dim\n",
        "        self.a_dim = a_dim\n",
        "        self.gamma = gamma\n",
        "        self.lamb = lamb\n",
        "        self.max_step = max_step\n",
        "        self.device = device\n",
        "\n",
        "        #Storages (state, action, value, reward, a_logp)\n",
        "        self.mb_states = np.zeros((self.max_step, self.s_dim), dtype=np.float32)\n",
        "        self.mb_actions = np.zeros((self.max_step, self.a_dim), dtype=np.float32)\n",
        "        self.mb_values = np.zeros((self.max_step,), dtype=np.float32)\n",
        "        self.mb_rewards = np.zeros((self.max_step,), dtype=np.float32)\n",
        "        self.mb_a_logps = np.zeros((self.max_step,), dtype=np.float32)\n",
        "\n",
        "    #Compute discounted return\n",
        "    def compute_discounted_return(self, rewards, last_value):\n",
        "        returns = np.zeros_like(rewards)\n",
        "        n_step = len(rewards)\n",
        "        for t in reversed(range(n_step)):\n",
        "            if t == n_step - 1:\n",
        "                returns[t] = rewards[t] + self.gamma*last_value\n",
        "            else:\n",
        "                returns[t] = rewards[t] + self.gamma*returns[t+1]\n",
        "\n",
        "        return returns\n",
        "\n",
        "    #Compute generalized advantage estimation (Optional)\n",
        "    def compute_gae(self, rewards, values, last_value):\n",
        "        advs = np.zeros_like(rewards)\n",
        "        n_step = len(rewards)\n",
        "        last_gae_lam = 0.0\n",
        "\n",
        "        for t in reversed(range(n_step)):\n",
        "            if t == n_step - 1:\n",
        "                next_value = last_value\n",
        "            else:\n",
        "                next_value = values[t+1]\n",
        "\n",
        "            delta = rewards[t] + self.gamma*next_value - values[t]\n",
        "            advs[t] = last_gae_lam = delta + self.gamma*self.lamb*last_gae_lam\n",
        "\n",
        "        return advs + values\n",
        "\n",
        "    #Run an episode using the policy net & value net\n",
        "    def run(self, env, policy_net, value_net):\n",
        "        #TODO(Lab-4): Run an episode to collect data\n",
        "        state = env.reset()\n",
        "        episode_len = self.max_step\n",
        "        for step in range(self.max_step):\n",
        "            state_tensor = torch.tensor(np.expand_dims(state, axis=0), dtype=torch.float32, device=self.device)\n",
        "            action, a_logp = policy_net.evaluate(state_tensor)\n",
        "            value =value_net(state_tensor)\n",
        "            action = action.cpu().numpy()[0]\n",
        "            a_logp = a_logp.cpu().numpy()\n",
        "            value = value.cpu().numpy()\n",
        "\n",
        "            self.mb_states[step] = state\n",
        "            self.mb_actions[step] = action\n",
        "            self.mb_values[step] = value\n",
        "            self.mb_a_logps[step] = a_logp\n",
        "\n",
        "            state, reward, done, info = env.step(action)\n",
        "            self.mb_rewards[step] = reward\n",
        "\n",
        "            if done:\n",
        "              episode_len = step + 1\n",
        "              break\n",
        "\n",
        "        #Compute returns\n",
        "        last_value = value_net(\n",
        "            torch.tensor(np.expand_dims(state, axis=0), dtype=torch.float32, device=self.device)\n",
        "        ).cpu().numpy()\n",
        "\n",
        "        mb_returns = self.compute_discounted_return(self.mb_rewards[:episode_len], last_value)\n",
        "        '''\n",
        "        mb_returns = self.compute_gae(\n",
        "            self.mb_rewards[:episode_len],\n",
        "            self.mb_values[:episode_len],\n",
        "            last_value\n",
        "        )\n",
        "        '''\n",
        "        return self.mb_states[:episode_len], \\\n",
        "                self.mb_actions[:episode_len], \\\n",
        "                self.mb_a_logps[:episode_len], \\\n",
        "                self.mb_values[:episode_len], \\\n",
        "                mb_returns, \\\n",
        "                self.mb_rewards[:episode_len]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mXvKD2FYlkf"
      },
      "source": [
        "### 3.2 Create EnvRunner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osGbTQQpoX3F"
      },
      "source": [
        "runner = EnvRunner(s_dim, a_dim)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ntP7LHFocXJ"
      },
      "source": [
        "## 4. PPO Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hgs5NUQGaVPf"
      },
      "source": [
        "### 4.1 PPO Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn5EsadHoiQg"
      },
      "source": [
        "class PPO:\n",
        "    #Constructor\n",
        "    def __init__(self, policy_net, value_net, lr=1e-4, max_grad_norm=0.5, ent_weight=0.01, clip_val=0.2, sample_n_epoch=4, sample_mb_size=64, device='cpu'):\n",
        "        self.policy_net = policy_net\n",
        "        self.value_net = value_net\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.ent_weight = ent_weight\n",
        "        self.clip_val = clip_val\n",
        "        self.sample_n_epoch = sample_n_epoch\n",
        "        self.sample_mb_size = sample_mb_size\n",
        "        self.device = device\n",
        "        self.opt_polcy = torch.optim.Adam(policy_net.parameters(), lr)\n",
        "        self.opt_value = torch.optim.Adam(value_net.parameters(), lr)\n",
        "\n",
        "    #Train the policy net & value net using PPO\n",
        "    def train(self, mb_states, mb_actions, mb_old_values, mb_advs, mb_returns, mb_old_a_logps):\n",
        "        #Convert numpy array to tensor\n",
        "        mb_states = torch.from_numpy(mb_states).to(self.device)\n",
        "        mb_actions = torch.from_numpy(mb_actions).to(self.device)\n",
        "        mb_old_values = torch.from_numpy(mb_old_values).to(self.device)\n",
        "        mb_advs = torch.from_numpy(mb_advs).to(self.device)\n",
        "        mb_returns = torch.from_numpy(mb_returns).to(self.device)\n",
        "        mb_old_a_logps = torch.from_numpy(mb_old_a_logps).to(self.device)\n",
        "        episode_length = len(mb_states)\n",
        "        rand_idx = np.arange(episode_length)\n",
        "        sample_n_mb = episode_length // self.sample_mb_size\n",
        "\n",
        "        if sample_n_mb <= 0:\n",
        "            sample_mb_size = episode_length\n",
        "            sample_n_mb = 1\n",
        "        else:\n",
        "            sample_mb_size = self.sample_mb_size\n",
        "\n",
        "        for i in range(self.sample_n_epoch):\n",
        "            np.random.shuffle(rand_idx)\n",
        "\n",
        "            for j in range(sample_n_mb):\n",
        "                #Randomly sample a batch for training\n",
        "                sample_idx = rand_idx[j*sample_mb_size : (j+1)*sample_mb_size]\n",
        "                sample_states = mb_states[sample_idx]\n",
        "                sample_actions = mb_actions[sample_idx]\n",
        "                sample_old_values = mb_old_values[sample_idx]\n",
        "                sample_advs = mb_advs[sample_idx]\n",
        "                sample_returns = mb_returns[sample_idx]\n",
        "                sample_old_a_logps = mb_old_a_logps[sample_idx]\n",
        "\n",
        "                sample_a_logps, sample_ents = self.policy_net.evaluate(sample_states, sample_actions)\n",
        "                sample_values = self.value_net(sample_states)\n",
        "                ent = sample_ents.mean()\n",
        "\n",
        "                #TODO(Lab-5): Compute value loss & policy gradient loss\n",
        "                v_pred_clip = sample_old_values + torch.clamp(sample_values - sample_old_values, -self.clip_val, self.clip_val)\n",
        "                v_loss1 = (sample_returns - sample_values) ** 2\n",
        "                v_loss2 = (sample_returns - v_pred_clip) ** 2\n",
        "                v_loss = torch.max(v_loss1, v_loss2).mean()\n",
        "\n",
        "                ratio = (sample_a_logps - sample_old_a_logps).exp()\n",
        "                pg_loss1 = -sample_advs * ratio\n",
        "                pg_loss2 = -sample_advs * torch.clamp(ratio ,1.0 - self.clip_val, 1.0 + self.clip_val)\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean() - self.ent_weight * ent\n",
        "\n",
        "                #Train actor\n",
        "                self.opt_polcy.zero_grad()\n",
        "                pg_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.max_grad_norm)\n",
        "                self.opt_polcy.step()\n",
        "\n",
        "                #Train critic\n",
        "                self.opt_value.zero_grad()\n",
        "                v_loss.backward()\n",
        "                nn.utils.clip_grad_norm_(self.value_net.parameters(), self.max_grad_norm)\n",
        "                self.opt_value.step()\n",
        "\n",
        "        return pg_loss.item(), v_loss.item(), ent.item()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npiu_IeAaa3m"
      },
      "source": [
        "### 4.2 Create PPO Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcDzxqhOoyzz"
      },
      "source": [
        "agent = PPO(policy_net, value_net)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu_Pukpbo1uw"
      },
      "source": [
        "## 5. Training and Testing Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfT0oUo7bNOo"
      },
      "source": [
        "### 5.1 Play an Episode for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOn7Ok6TpE85"
      },
      "source": [
        "def play(policy_net):\n",
        "    render_env = Recorder(gym.make('BipedalWalker-v3'), project_root + '/video')\n",
        "\n",
        "    with torch.no_grad():\n",
        "      state = render_env.reset()\n",
        "      length = 0\n",
        "      total_reward = 0\n",
        "      while not done:\n",
        "        state_tensor = torch.tensor(np.expand_dims(state, axis=0), dtype=torch.float32, device='cpu')\n",
        "        action = policy_net.choose_action(state_tensor, deterministic=True).cpu().numpy()\n",
        "        state, reward, done, info = render_env.step(action[0])\n",
        "        total_reward += reward\n",
        "        length += 1\n",
        "        if done:\n",
        "          print(\"[Evaluation] Total reward = {:.6f},length{:d}\".formate(total_reward,length),flush=True)\n",
        "\n",
        "\n",
        "    render_env.play()\n",
        "    render_env.close()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwhxKCl_pp4y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "fbeed0a2-d7c6-408d-fbe7-e15e514dbc6d"
      },
      "source": [
        "play(policy_net)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/ppo_tutorial//video'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-2254790617.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-18-3987339547.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(policy_net)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrender_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BipedalWalker-v3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/video'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colabgymrender/recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, directory, fps)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/ppo_tutorial//video'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Te32jFabU4v"
      },
      "source": [
        "### 5.2 Train the Networks using PPO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XnleKrKpcbI"
      },
      "source": [
        "def train(env, runner, policy_net, value_net, agent, max_episode=5000):\n",
        "    mean_total_reward = 0\n",
        "    mean_length = 0\n",
        "    save_dir = project_root + '/save'\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.mkdir(save_dir)\n",
        "\n",
        "    for i in range(max_episode):\n",
        "        mean_total_reward = 0\n",
        "        mean_length = 0\n",
        "        save_dir = project_root + '/save'\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.mkdir(save_dir)\n",
        "        for i in range(max_episode):\n",
        "            with torch.no_grad():\n",
        "                mb_states, mb_actions, mb_old_a_logps, mb_values, mb_returns, mb_rewards = runner.run(env, policy_net, value_net)\n",
        "                mb_advs = mb_returns - mb_values\n",
        "                mb_advs = (mb_advs - mb_advs.mean()) / (mb_advs.std() + 1e-6)\n",
        "\n",
        "            pg_loss, v_loss, ent = agent.train(mb_states, mb_actions, mb_values, mb_advs, mb_returns, mb_old_a_logps)\n",
        "            mean_total_reward += mb_rewards.sum()\n",
        "            mean_length += len(mb_states)\n",
        "            print(\"[Episode {:4d}] total reward = {:.6f}, length = {:d}\".format(i, mb_rewards.sum(), len(mb_states)))\n",
        "\n",
        "        #Show the current result & save the model\n",
        "        if i % 200 == 0:\n",
        "            print(\"\\n[{:5d} / {:5d}]\".format(i, max_episode))\n",
        "            print(\"----------------------------------\")\n",
        "            print(\"actor loss = {:.6f}\".format(pg_loss))\n",
        "            print(\"critic loss = {:.6f}\".format(v_loss))\n",
        "            print(\"entropy = {:.6f}\".format(ent))\n",
        "            print(\"mean return = {:.6f}\".format(mean_total_reward / 200))\n",
        "            print(\"mean length = {:.2f}\".format(mean_length / 200))\n",
        "            print(\"\\nSaving the model ... \", end=\"\")\n",
        "            torch.save({\n",
        "                \"it\": i,\n",
        "                \"PolicyNet\": policy_net.state_dict(),\n",
        "                \"ValueNet\": value_net.state_dict()\n",
        "            }, os.path.join(save_dir, \"model.pt\"))\n",
        "            print(\"Done.\")\n",
        "            print()\n",
        "            play(policy_net)\n",
        "            mean_total_reward = 0\n",
        "            mean_length = 0"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVFcv-pOpujx"
      },
      "source": [
        "train(env, runner, policy_net, value_net, agent)\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59tHFUVFbsTR"
      },
      "source": [
        "### 5.3 Load the Model and Play"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrqK62jpqF9B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "463d783f-c688-4ae0-ab2d-13971e3c5cbb"
      },
      "source": [
        "save_dir = project_root + '/save'\n",
        "model_path = os.path.join(save_dir, \"model.pt\")\n",
        "\n",
        "if os.path.exists(model_path):\n",
        "    print(\"Loading the model ... \", end=\"\")\n",
        "    checkpoint = torch.load(model_path)\n",
        "    policy_net.load_state_dict(checkpoint[\"PolicyNet\"])\n",
        "    print(\"Done.\")\n",
        "else:\n",
        "    print('ERROR: No model saved')\n",
        "\n",
        "play(policy_net)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: No model saved\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/ppo_tutorial//video'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-515597647.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR: No model saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-18-3987339547.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(policy_net)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrender_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BipedalWalker-v3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_root\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/video'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrender_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/colabgymrender/recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, directory, fps)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/ppo_tutorial//video'"
          ]
        }
      ]
    }
  ]
}